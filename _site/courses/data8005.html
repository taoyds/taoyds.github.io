<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>DATA8005 | Tao Yu (余涛)</title>
  <meta name="description" content="Advanced Natural Language Processing">
  <meta name="author" content="Tao Yu">
  <meta property="og:title" content="Tao Yu" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://taoyds.github.io/" />
  <meta property="og:site_name" content="Tao Yu" />
  <link rel="canonical" href="https://taoyds.github.io/" />

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600' rel='stylesheet' type='text/css'>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton/normalize.css>
  <link rel="stylesheet" href=/libs/external/skeleton/skeleton.css>
  <link rel="stylesheet" href=/libs/custom/my_css.css>
  <link rel="stylesheet" href=/libs/custom/course.css>

  <!-- JQuery
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src=/libs/external/jquery-3.1.1.min.js></script>

  <!-- Font-Awesome
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/font-awesome-4.7.0/css/font-awesome.min.css>

  <!-- Academicons
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/academicons-1.8.6/css/academicons.min.css>

  <!-- Skeleton tabs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton_tabs/skeleton-tabs.css>
  <script src=/libs/external/skeleton_tabs/skeleton-tabs.js></script>

  <!-- Timeline
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/timeline.css>

  <!-- Scripts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!--<link rel="stylesheet" href=/libs/external/github-prettify-theme.css>-->
  <script src=/libs/custom/my_js.js></script>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href=/libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=/libs/icon.png>

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="course">
      <h1 id="data8005-advanced-natural-language-processing">DATA8005: Advanced Natural Language Processing</h1>

<h2 id="course-information">Course Information</h2>

<p>Instructor</p>

<ul>
  <li><a href="https://taoyds.github.io/">Tao Yu</a></li>
  <li>Office Hour: Tuesday 4 - 5pm @CB 204E</li>
</ul>

<p>Lecture</p>

<ul>
  <li>Friday 1:30 - 4:20pm @IDS P603</li>
</ul>

<h2 id="course-description">Course Description</h2>

<p>Natural language processing (NLP) is the study of human language from a computational perspective. This course is an introductory graduate-level course on natural language processing aimed at students who are interested in doing cutting-edge research in the field. In this class, we will cover recent developments on core techniques and modern advances in NLP, especially in the era of large language models. We will also survey some recent NLP research topics including language grounding, agents, multimodality, interactivity, and interoperability for NLP. Students will gain the necessary skills and experience to understand, design, implement, and test large language models through a final project. We will also introduce cutting-edge research topics and learn how to conduct NLP research through paper readings and discussions. We will potentially also host invited speakers for talks.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>We require students to have prior knowledge undergraduate linear algebra, probability and statistics, machine learning, or deep learning. Familiarity with Python programming is required. Introduction to natural language processing is recommended.</p>

<h2 id="course-materials">Course Materials</h2>

<p>There is no required textbook for this course (<a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Natural Language Processing</a> by Jacob Eisenstein is recommended if you would like to read more about NLP). Readings from papers, blogs, tutorials, and book chapters will be posted on the course website. If you don’t have a background in NLP, you may find the introductory undergraduate-level NLP course <a href="https://taoyds.github.io/courses/comp3361">COMP 3361</a> helpful.</p>

<h2 id="grading">Grading</h2>

<ul>
  <li>30%: <strong>Class presentation</strong>
    <ul>
      <li>You (a group of two) will present on the ~2-3 papers on an NLP topic assigned for a particular day (~60 minutes). We will do scheduling and signups at the beginning of the semester. The goal is to educate the others in the class about the topic, so do think about how to best cover the material (survey and review more papers/blogs on the topic), do a good job with slides, and be prepared for lots of questions.</li>
      <li>Papers will be chosen for each topic (ToD). You are still welcome to suggest relevant papers that you like to present (and coordinate with the instructor). It is your job to decide what to cover in the lecture and how to divide the work with your partner.</li>
      <li>You are also required to meet with the instructor before the lecture. Please send your draft slides via email before the meeting and we will go over your slides during the meeting.</li>
    </ul>
  </li>
  <li>25%: <strong>Course participation</strong>
    <ul>
      <li>Each class will cover 2 topics. You are expected to read these papers before class, add in comments (answer, ask, or add &gt;2 high-quality questions/comments, suggest related papers). Please come to the class prepared with several points that will substantially contribute to the group discussion. Your participation grade will be determined based on attendance and more importantly, substantial contributions to paper discussions both on Slack and in class.</li>
    </ul>
  </li>
  <li>45%: <strong>Final project</strong>
    <ul>
      <li>The final project is an opportunity for open-ended exploration of concepts in the course. This project should constitute novel work beyond directly implementing concepts from lecture and should result in a report that roughly reads like an NLP/ML conference submission in terms of presentation and scope. You can work on the final project in a group of 2-3. You are allowed to integrate the project with ongoing research or projects from other classes (assuming the other instructor also approves). Here are a few example project tasks, such as <a href="https://os-world.github.io">OSWorld</a>, <a href="https://www.swebench.com">SWE-bench</a>, and <a href="https://spider2-sql.github.io">Spider 2.0</a>.</li>
      <li><strong>Deliverables</strong>
        <ul>
          <li>10% <strong>Proposal</strong>: Partway through the semester, you will submit a brief proposal (around 1 page) explaining your idea and plan including what problem or task you want to address, what dataset(s) you want to work on, what metrics you need to employ, what baselines you would like to compare with.</li>
          <li>35% <strong>Writeup</strong>: Your final project report should be 4-8 pages—use your discretion about the length. Groups of two should have reports closer to 8 pages. The scope should be similar to that of an ACL paper (<a href="https://github.com/ICLR/Master-Template/raw/master/iclr2024.zip">ICLR style files</a>). You should present a novel idea, discuss related work, describe your implementation or what you did, give results, and provide discussion or error analysis. Due: 11:59 pm on Dec 15</li>
          <li><strong>Presentation</strong>: You will give lightning talks (around 3 minutes) about your project (ToD).</li>
        </ul>
      </li>
      <li>More guidelines will be announced soon.</li>
    </ul>
  </li>
</ul>

<h2 id="course-schedule">Course Schedule</h2>

<table class="table">
<colgroup>
    <col style="width:10%" />
    <col style="width:20%" />
    <col style="width:40%" />
    <col style="width:10%" />
    <col style="width:10%" />
</colgroup>
<thead>
<tr>
    <th>Date</th>
    <th>Topic</th>
    <th>Material</th>
    <th>Event</th>
    <th>Due</th>
</tr>
</thead>
<tbody>
    
    
    
    <tr>
      <td><b><font color="red">Week 1</font></b><br />Sep 6</td>
      <td>
      
        
          <div>
            Canceled due to bad weather
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 2</font></b><br />Sep 13</td>
      <td>
      
        
          <div>
            Introduction (Tao Yu)
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec01.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://www.amacad.org/publication/human-language-understanding-reasoning">Human Language Understanding &amp; Reasoning</a></li>
              
              <li><a href="https://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf">M. Collins, Notes 1</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 3</font></b><br />Sep 20</td>
      <td>
      
        
          <div>
            Introduction to LLMs (Tao Yu)
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec02.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf">J &amp; M 7.3-7.5</a></li>
              
              <li><a href="https://arxiv.org/abs/1508.07909">Byte-pair encoding paper</a></li>
              
              <li><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a></li>
              
              <li><a href="https://arxiv.org/pdf/2005.14165.pdf">Language Models are Few-Shot Learners (GPT-3)</a></li>
              
            </ul>
          
            
              Others
            
            <ul>
              
              <li><a href="https://www.youtube.com/watch?v=zduSFxRajkE">Talk on the GPT Tokenizer by Andrej Karpathy</a></li>
              
              <li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
              
              <li><a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>
              
              <li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Talk on building GPT&#58; from scratch, in code, spelled out by Andrej Karpathy</a></li>
              
            </ul>
          
        
    </td>
      <td><a href="https://docs.google.com/spreadsheets/d/1Cb531jMBokaOoUT807h3E8Rp3EWgYTu5zQehoAc3bVs/edit?usp=sharing"> Registration</a> Out</td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 4</font></b><br />Sep 27</td>
      <td>
      
        
          <div>
            Introduction to LLMs (Tao Yu)
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec03-1.pdf">slides</a>]
            
          </div>
        
          <div>
            The Llama 3 Herd of Models
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec03-2.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2407.21783">The Llama 3 Herd of Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2408.00118">Gemma 2 &#58; Improving Open Language Models at a Practical Size</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 5</font></b><br />Oct 4</td>
      <td>
      
        
          <div>
            LM post-training 2&#58; SFT, instruction tuning (Sihui Ji, Tianzhe Chu)
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec04-1.pdf">slides</a>]
            
          </div>
        
          <div>
            LM data and evaluation (Jianrui Wu, Tianle Li)
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec04-2.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2407.21783">The Llama 3 Herd of Models</a> (post-training part only)</li>
              
              <li><a href="https://arxiv.org/abs/2212.10560">Self-Instruct &#58; Aligning Language Model with Self-Generated Instructions</a></li>
              
              <li><a href="https://arxiv.org/abs/2110.08207">Multitask Prompted Training Enables Zero-Shot Task Generalization</a></li>
              
              <li><a href="https://arxiv.org/abs/2406.17557">The FineWeb Datasets &#58; Decanting the Web for the Finest Text Data at Scale</a></li>
              
              <li><a href="https://arxiv.org/abs/2305.10429">DoReMi &#58; Optimizing Data Mixtures Speeds Up Language Model Pretraining</a></li>
              
              <li><a href="https://arxiv.org/abs/2403.04132">Chatbot Arena &#58; An Open Platform for Evaluating LLMs by Human Preference</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td><a href="https://docs.google.com/spreadsheets/d/1Cb531jMBokaOoUT807h3E8Rp3EWgYTu5zQehoAc3bVs/edit?usp=sharing">Project registration</a> <font color="red"><b>Due</b></font></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 6</font></b><br />Oct 11</td>
      <td>
      
        
          <div>
            No class
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 7</font></b><br />Oct 18</td>
      <td>
      
        
          <div>
            No class
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 8</font></b><br />Oct 25</td>
      <td>
      
        
          <div>
            LM safety, bias, and privacy (Yifeng Lin, Pinglu Gong, Fengyi Xu)
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec05-1.pdf">slides</a>]
            
          </div>
        
          <div>
            LM post-training 2&#58; alignment, RLHF/DPO (Runhui Huang, Yiyang Wang)
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec05-2.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2310.08419">Jailbreaking Black Box Large Language Models in Twenty Queries</a></li>
              
              <li><a href="https://arxiv.org/abs/2012.07805">Extracting Training Data from Large Language Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a></li>
              
              <li><a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a></li>
              
              <li><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization 2&#58; Your Language Model is Secretly a Reward Model</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td>Project proposal <font color="red"><b>Due</b></font></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 9</font></b><br />Nov 1</td>
      <td>
      
        
          <div>
            Efficient LM adaptation (Sidi Yang, Yatai Ji, Jing Xiong)
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec06-1.pdf">slides</a>]
            
          </div>
        
          <div>
            Efficient LM training (Qi Guicheng, Shen Che, Zijian Ye)
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec06-2.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2106.09685">LoRA &#58; Low-Rank Adaptation of Large Language Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2305.14314">QLoRA &#58; Efficient Finetuning of Quantized LLMs</a></li>
              
              <li><a href="https://arxiv.org/abs/2405.09673">LoRA Learns Less and Forgets Less</a></li>
              
              <li><a href="https://arxiv.org/abs/2205.14135">FlashAttention &#58; Fast and Memory-Efficient Exact Attention with IO-Awareness</a></li>
              
              <li><a href="https://arxiv.org/abs/2312.00752">Mamba &#58; Linear-Time Sequence Modeling with Selective State Spaces</a></li>
              
              <li><a href="https://github.com/stanford-cs336/spring2024-lectures/blob/main/nonexecutable/Lecture%204%20-%20details%20%2B%20MoEs.pdf">Overview on Mixture of Experts</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 10</font></b><br />Nov 8</td>
      <td>
      
        
          <div>
            Multimodal LMs 1 (Chenming Zhu, Pei Zhou, Yi Zhang)
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec07-1.pdf">slides</a>]
            
          </div>
        
          <div>
            Multimodal LMs 2 (Mengzhao Chen, Tianshuo Yang, Chengqi Duan)
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec07-2.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a></li>
              
              <li><a href="https://arxiv.org/abs/2204.14198">Flamingo &#58; a Visual Language Model for Few-Shot Learning</a></li>
              
              <li><a href="https://arxiv.org/abs/2304.08485">Visual Instruction Tuning</a></li>
              
              <li><a href="https://arxiv.org/abs/2409.12191">Qwen2-VL &#58; Enhancing Vision-Language Model Perception of the World at Any Resolution</a></li>
              
              <li><a href="https://arxiv.org/abs/2405.09818">Chameleon &#58; Mixed-Modal Early-Fusion Foundation Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2112.10741">GLIDE &#58; Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 11</font></b><br />Nov 15</td>
      <td>
      
        
          <div>
            LLM/VLMs + Robotics 1 (Feng Chen, Ruizhe Liu)
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec08-1.pdf">slides</a>]
            
          </div>
        
          <div>
            LLM/VLMs + Robotics 2 (Yi Chen, Lu Qiu)
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec08-2.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://gen-sim.github.io/">GenSim &#58; Generating Robotic Simulation Tasks via Large Language Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2311.01455">RoboGen &#58; Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation</a></li>
              
              <li><a href="https://rlvlmf2024.github.io/">RL-VLM-F &#58; Reinforcement Learning from Vision Language Foundation Model Feedback</a></li>
              
              <li><a href="https://openvla.github.io/">OpenVLA &#58; An Open-Source Vision-Language-Action Model</a></li>
              
              <li><a href="https://latentactionpretraining.github.io/">LAPA &#58; Latent Action Pretraining from Videos</a></li>
              
              <li><a href="https://say-can.github.io/">Do As I Can, Not As I Say &#58; Grounding Language in Robotic Affordances</a></li>
              
              <li><a href="https://arxiv.org/abs/2402.15391">Genie &#58; Generative Interactive Environments</a></li>
              
              <li><a href="https://arxiv.org/abs/2307.15818">RT-2 &#58; Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</a></li>
              
              <li><a href="https://generative-value-learning.github.io/">Vision Language Models are In-Context Value Learners</a></li>
              
            </ul>
          
            
              Others
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2312.08782">Toward General-Purpose Robots via Foundation Models &#58; A Survey and Meta-Analysis</a></li>
              
              <li><a href="https://www.youtube.com/live/Qhxr0uVT2zs">Project GR00T &#58; A Blueprint for Generalist Robotics - Jim Fan, NVIDIA </a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 12</font></b><br />Nov 22</td>
      <td>
      
        
          <div>
            LLM/VLMs as Agents (Xinyuan Wang, Bowen Wang)
            
              <br />
              [<a href="/assets/courses/DATA8005-24fall-lec09-1.pdf">slides</a>]
            
          </div>
        
          <div>
            Agents in the digital and physical world (Tao Yu)
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2210.03629">ReAct &#58; Synergizing Reasoning and Acting in Language Models</a></li>
              
              <li><a href="https://os-world.github.io/">OSWorld &#58; Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments</a></li>
              
              <li><a href="https://arxiv.org/abs/2411.02391">Attacking Vision-Language Computer Agents via Pop-ups</a></li>
              
              <li><a href="https://arxiv.org/abs/2206.11795">Video PreTraining (VPT) &#58; Learning to Act by Watching Unlabeled Online Videos</a></li>
              
              <li><a href="https://language-agent-tutorial.github.io/">EMNLP 2024 Tutorial on Language Agents &#58; Foundations, Prospects, and Risks</a></li>
              
              <li><a href="https://arxiv.org/abs/2407.01502">AI Agents That Matter</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 13</font></b><br />Nov 29</td>
      <td>
      
        
          <div>
            Embodied AI (Guest lecture &#58; Yanchao Yang)
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 15</font></b><br />Dec 6</td>
      <td>
      
        
          <div>
            No class
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 16</font></b><br />Dec 13</td>
      <td>
      
        
          <div>
            No class
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    

</tbody>
</table>

    </div>

    <div class="footer">
      <div class="row">
        <div class="four columns">
          Tao Yu (余涛)
        </div>
        <div class="four columns">
          tao.yu.nlp [AT] gmail.com
        </div>
        <div class="four columns">
          <span onclick="window.open('https://twitter.com/taoyds')" style="cursor: pointer">
            <i class="fa fa-twitter" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://www.linkedin.com/in/tao-yu-b9b551a5/')" style="cursor: pointer">
            <i class="fa fa-linkedin-square" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://github.com/taoyds')" style="cursor: pointer">
            <i class="fa fa-github" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://scholar.google.com/citations?user=5_Fn5CIAAAAJ&hl')" style="cursor: pointer">
            <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
          </span>
        </div>
      </div>
    </div>

  </div>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-204806507-1', 'auto');
  ga('send', 'pageview');

</script>

  <!-- do not remove
  <span id="62cd7b7da1aff3196fdc26b60e396df9"></span>
   -->

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
