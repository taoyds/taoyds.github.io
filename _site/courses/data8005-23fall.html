<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>DATA8005 | Tao Yu (余涛)</title>
  <meta name="description" content="Advanced Natural Language Processing">
  <meta name="author" content="Tao Yu">
  <meta property="og:title" content="Tao Yu" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://taoyds.github.io/" />
  <meta property="og:site_name" content="Tao Yu" />
  <link rel="canonical" href="https://taoyds.github.io/" />

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600' rel='stylesheet' type='text/css'>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton/normalize.css>
  <link rel="stylesheet" href=/libs/external/skeleton/skeleton.css>
  <link rel="stylesheet" href=/libs/custom/my_css.css>
  <link rel="stylesheet" href=/libs/custom/course.css>

  <!-- JQuery
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src=/libs/external/jquery-3.1.1.min.js></script>

  <!-- Font-Awesome
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/font-awesome-4.7.0/css/font-awesome.min.css>

  <!-- Academicons
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/academicons-1.8.6/css/academicons.min.css>

  <!-- Skeleton tabs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton_tabs/skeleton-tabs.css>
  <script src=/libs/external/skeleton_tabs/skeleton-tabs.js></script>

  <!-- Timeline
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/timeline.css>

  <!-- Scripts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!--<link rel="stylesheet" href=/libs/external/github-prettify-theme.css>-->
  <script src=/libs/custom/my_js.js></script>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href=/libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=/libs/icon.png>

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="course">
      <h1 id="data8005-advanced-natural-language-processing">DATA8005: Advanced Natural Language Processing</h1>

<h2 id="course-information">Course Information</h2>

<p>Instructor</p>

<ul>
  <li><a href="https://taoyds.github.io/">Tao Yu</a></li>
  <li>Office Hour: Wednesday 4 - 5pm @<a href="https://datascience.hku.hk/contact-us/#find-us">IDS P307H</a></li>
</ul>

<p>Lecture</p>

<ul>
  <li>Friday 1:30 - 4:20pm @Learning Commons CPD 2.25</li>
</ul>

<h2 id="course-description">Course Description</h2>

<p>Natural language processing (NLP) is the study of human language from a computational perspective. This course is an introductory graduate-level course on natural language processing aimed at students who are interested in doing cutting-edge research in the field. In this class, we will cover recent developments on core techniques and modern advances in NLP, especially in the era of large language models. We will also survey some recent NLP research topics including language grounding, agents, multimodality, interactivity, and interoperability for NLP. Students will gain the necessary skills and experience to understand, design, implement, and test large language models through a final project. We will also introduce cutting-edge research topics and learn how to conduct NLP research through paper readings and discussions. We will potentially also host invited speakers for talks.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>We require students to have prior knowledge undergraduate linear algebra, probability and statistics, machine learning, or deep learning. Familiarity with Python programming is required. Introduction to natural language processing is recommended.</p>

<h2 id="course-materials">Course Materials</h2>

<p>There is no required textbook for this course (<a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Natural Language Processing</a> by Jacob Eisenstein is recommended if you would like to read more about NLP). Readings from papers, blogs, tutorials, and book chapters will be posted on the course website.</p>

<h2 id="grading">Grading</h2>

<ul>
  <li>30%: <strong>Class presentation</strong>
    <ul>
      <li>You (a group of two) will present on the ~2-3 papers on an NLP topic assigned for a particular day (~60 minutes). We will do scheduling and signups at the beginning of the semester. The goal is to educate the others in the class about the topic, so do think about how to best cover the material (survey and review more papers/blogs on the topic), do a good job with slides, and be prepared for lots of questions.</li>
      <li>Papers will be chosen for each topic (ToD). You are still welcome to suggest relevant papers that you like to present (and coordinate with the instructor). It is your job to decide what to cover in the lecture and how to divide the work with your partner.</li>
      <li>You are also required to meet with the instructor before the lecture. Please send your draft slides via email before the meeting and we will go over your slides during the meeting.</li>
    </ul>
  </li>
  <li>
    <p>25%: <strong>Course participation</strong></p>

    <ul>
      <li>Each class will cover 2 topics. You are expected to read these papers before class, add in comments (answer, ask, or add &gt;2 high-quality questions/comments, suggest related papers). Please come to the class prepared with several points that will substantially contribute to the group discussion. Your participation grade will be determined based on attendance and more importantly, substantial contributions to paper discussions both on Slack and in class.</li>
    </ul>
  </li>
  <li>45%: <strong>Final project</strong>
    <ul>
      <li>The final project is an opportunity for open-ended exploration of concepts in the course. This project should constitute novel work beyond directly implementing concepts from lecture and should result in a report that roughly reads like an NLP/ML conference submission in terms of presentation and scope. You can work on the final project in a group of 2-3. You are allowed to integrate the project with ongoing research or projects from other classes (assuming the other instructor also approves).</li>
      <li><strong>Deliverables</strong>
        <ul>
          <li>10% <strong>Proposal</strong>: Partway through the semester, you will submit a brief proposal (around 1 page) explaining your idea and plan including what problem or task you want to address, what dataset(s) you want to work on, what metrics you need to employ, what baselines you would like to compare with. Project group registration: week 5; proposal due: week 7.</li>
          <li>35% <strong>Writeup</strong>: Your final project report should be 4-8 pages—use your discretion about the length. Groups of two should have reports closer to 8 pages. The scope should be similar to that of an ACL paper (<a href="https://github.com/ICLR/Master-Template/raw/master/iclr2024.zip">ICLR style files</a>). You should present a novel idea, discuss related work, describe your implementation or what you did, give results, and provide discussion or error analysis. Due: 11:59 pm on Dec 15</li>
          <li><strong>Presentation</strong>: You will give lightning talks (around 3 minutes) about your project (ToD).</li>
        </ul>
      </li>
      <li>More guidelines will be announced soon.</li>
    </ul>
  </li>
</ul>

<h2 id="course-schedule">Course Schedule</h2>

<table class="table">
<colgroup>
    <col style="width:10%" />
    <col style="width:20%" />
    <col style="width:40%" />
    <col style="width:10%" />
    <col style="width:10%" />
</colgroup>
<thead>
<tr>
    <th>Date</th>
    <th>Topic</th>
    <th>Material</th>
    <th>Event</th>
    <th>Due</th>
</tr>
</thead>
<tbody>
    
    
    
    <tr>
      <td><b><font color="red">Week 1</font></b><br />Sep 1</td>
      <td>
      
        
          <div>
            Canceled due to bad weather
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 2</font></b><br />Sep 8</td>
      <td>
      
        
          <div>
            Canceled due to bad weather
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 3</font></b><br />Sep 15</td>
      <td>
      
        
          <div>
            Introduction (Tao Yu)
            
              <br />
              [<a href="/assets/courses/DATA8005-Fall-2023-ANLP-L1.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td><a href="https://docs.google.com/spreadsheets/d/1rZSBtxw2r01X4bWQjffx8PAWzn8UNDZX-dIoUFhBxIk/edit?usp=sharing">Project registration</a> Out</td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 4</font></b><br />Sep 22</td>
      <td>
      
        
          <div>
            Introduction (Tao Yu)
            
              <br />
              [<a href="/assets/courses/DATA8005-Fall-2023-ANLP-L2.pdf">slides</a>]
            
          </div>
        
          <div>
            Pretraining (Shansan Gong)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec01.pdf">slides</a>]
            
          </div>
        
          <div>
            Prompting, in-context learning (Lei Li)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec02.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Paper Presentation
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></li>
              
              <li><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a></li>
              
              <li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2202.12837">Rethinking the Role of Demonstrations&#58; What Makes In-Context Learning Work?</a></li>
              
            </ul>
          
            
              Recommended Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
              
              <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
              
              <li><a href="http://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer</a></li>
              
              <li><a href="https://huggingface.co/learn/nlp-course/chapter1/1">HuggingFace's course on Transformers</a></li>
              
              <li><a href="https://arxiv.org/abs/2307.09288">Llama 2&#58; Open Foundation and Fine-Tuned Chat Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2102.09690">Calibrate Before Use&#58; Improving Few-Shot Performance of Language Models</a></li>
              
              <li><a href="https://cs.brynmawr.edu/Courses/cs372/spring2012/slides/02_IntelligentAgents.pdf">Intelligent Agents Russell &amp; Norvig Chapter 2</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 5</font></b><br />Sep 29</td>
      <td>
      
        
          <div>
            Reasoning, prompting (Tao Yu)
            
              <br />
              [<a href="/assets/courses/DATA8005-Fall-2023-ANLP-L3.pdf">slides</a>]
            
          </div>
        
          <div>
            Instruction tuning (Xubin Ren, Yiming Zhang)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec03.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Paper Presentation
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2203.11171">Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2206.07682">Emergent Abilities of Large Language Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2110.08207">Multitask Prompted Training Enables Zero-Shot Task Generalization</a></li>
              
              <li><a href="https://arxiv.org/abs/2212.10560">Self-Instruct&#58; Aligning Language Model with Self-Generated Instructions</a></li>
              
            </ul>
          
            
              Recommended Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2209.01975">Selective Annotation Makes Language Models Better Few-Shot Learners</a></li>
              
              <li><a href="https://arxiv.org/abs/2211.10435">PAL&#58; Program-aided Language Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2306.04751">How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources</a></li>
              
              <li><a href="https://instructor-embedding.github.io/">One Embedder, Any Task&#58; Instruction-Finetuned Text Embeddings</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td><a href="https://docs.google.com/spreadsheets/d/1rZSBtxw2r01X4bWQjffx8PAWzn8UNDZX-dIoUFhBxIk/edit?usp=sharing">Project registration</a> <font color="red"><b>Due</b></font></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 6</font></b><br />Oct 6</td>
      <td>
      
        
          <div>
            LM evaluation, data, and benchmarking (A: Yuanpeng Tu, Zhuoling Li)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec04.pdf">slides</a>]
            
          </div>
        
          <div>
            Alignment/RLHF (B: Tonghuan Xiao, Yangtuan Sun, Guichao Zhu)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec05.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Paper Presentation
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2206.04615">Beyond the Imitation Game&#58; Quantifying and extrapolating the capabilities of language models</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2101.00027">The Pile&#58; An 800GB Dataset of Diverse Text for Language Modeling</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2205.10487">Scaling Laws and Interpretability of Learning from Repeated Data</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a> (B)</li>
              
              <li><a href="https://arxiv.org/abs/2204.05862">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</a> (B)</li>
              
            </ul>
          
            
              Recommended Readings
            
            <ul>
              
              <li><a href="https://lmsys.org/blog/2023-06-22-leaderboard/">Vicuna leaderboard</a> (A)</li>
              
              <li><a href="https://stanford-cs324.github.io/winter2022/lectures/data/">Data for Training LLMs</a> (A)</li>
              
              <li><a href="https://huyenchip.com/2023/05/02/rlhf.html">RLHF&#58; Reinforcement Learning from Human Feedback</a> (B)</li>
              
              <li><a href="https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf">Prompting, Instruction Finetuning, and RLHF</a> (B)</li>
              
              <li><a href="https://mediusdownload.event.microsoft.com/video-51378/ea11c3c20e/BRK216HFS_v1.mp4?sv=2018-03-28&amp;sr=c&amp;sig=tlIPwp2z6q8TNAEig%2BOQGh4lL8o8hAHcdw33msvikXY%3D&amp;se=2028-05-24T06%3A23%3A01Z&amp;sp=r">State of GPT and RLHF LLMs - Andrej Karpathy, OpenAI</a> (B)</li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 7</font></b><br />Oct 13</td>
      <td>
      
        
          <div>
            Robustness, interpretability, explainability (A: Yunchao Zhang, Li Sun)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec06.pdf">slides</a>]
            
          </div>
        
          <div>
            Bias, toxicity, and privacy in LM (B: Mengkang Hu, Likai Peng)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec07.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Paper Presentation
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2207.05221">Language Models (Mostly) Know What They Know</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2211.13892">Complementary Explanations for Effective In-Context Learning</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2208.14271">Faithful Reasoning Using Large Language Models</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2012.07805">Extracting Training Data from Large Language Models</a> (B)</li>
              
              <li><a href="https://arxiv.org/abs/2009.11462">RealToxicityPrompts&#58; Evaluating Neural Toxic Degeneration in Language Models</a> (B)</li>
              
              <li><a href="https://arxiv.org/abs/2110.05679">Large Language Models Can Be Strong Differentially Private Learners</a> (B)</li>
              
            </ul>
          
            
              Recommended Readings
            
            <ul>
              
              <li><a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">Towards Monosemanticity&amp;#58 Decomposing Language Models With Dictionary Learning</a> (A)</li>
              
              <li><a href="https://stanford-cs324.github.io/winter2022/lectures/harms-1/">Harms From Data (Part 1)</a> (B)</li>
              
              <li><a href="https://stanford-cs324.github.io/winter2022/lectures/harms-2/">Harms From Data (Part 2)</a> (B)</li>
              
              <li><a href="https://stanford-cs324.github.io/winter2022/assets/pdfs/Privacy%20pdf.pdf">Security and Privacy</a> (B)</li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td>Project proposal <font color="red"><b>Due</b></font></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 8</font></b><br />Oct 20</td>
      <td>
      
        
          <div>
            No class
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 9</font></b><br />Oct 27</td>
      <td>
      
        
          <div>
            Parameter-efficient LM tuning (A: Qihang Fang, Jiannan Wang)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec08.pdf">slides</a>]
            
          </div>
        
          <div>
            Efficient LM methods and Infrastructure (B: Chenxin An, Yazheng Yang)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec09.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Paper Presentation
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2106.09685">LoRA&#58; Low-Rank Adaptation of Large Language Models</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2305.14314">QLoRA&#58; Efficient Finetuning of Quantized LLMs</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2210.17323">GPTQ&#58; Accurate Post-Training Quantization for Generative Pre-trained Transformers</a> (B)</li>
              
              <li><a href="https://arxiv.org/abs/2208.07339">LLM.int8()&#58; 8-bit Matrix Multiplication for Transformers at Scale</a> (B)</li>
              
              <li><a href="https://arxiv.org/abs/2205.14135">FlashAttention&#58; Fast and Memory-Efficient Exact Attention with IO-Awareness</a> (B)</li>
              
            </ul>
          
            
              Recommended Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2110.04366">Towards a Unified View of Parameter-Efficient Transfer Learning</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2110.04366">The Power of Scale for Parameter-Efficient Prompt Tuning</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2309.06180">Efficient Memory Management for Large Language Model Serving with PagedAttention</a> (B)</li>
              
              <li><a href="https://arxiv.org/abs/1909.08053">Megatron-LM&#58; Training Multi-Billion Parameter Language Models Using Model Parallelism</a> (B)</li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 10</font></b><br />Nov 3</td>
      <td>
      
        
          <div>
            Sparse/Retrieval-based LM (A: Lihe Yang, Meng Wei)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec10.pdf">slides</a>]
            
          </div>
        
          <div>
            Code LM (B: Ge Qu, Jinyang Li)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec11.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Paper Presentation
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2208.03306">Branch-Train-Merge&#58; Embarrassingly Parallel Training of Expert Language Models</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2112.04426">Improving language models by retrieving from trillions of tokens</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a> (B)</li>
              
              <li><a href="https://ds1000-code-gen.github.io/">DS-1000&#58; A Natural and Reliable Benchmark for Data Science Code Generation</a> (B)</li>
              
            </ul>
          
            
              Recommended Readings
            
            <ul>
              
              <li><a href="https://acl2023-retrieval-lm.github.io/">Tutorial&#58; Retrieval-based Language Models and Applications</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2304.05128">Teaching Large Language Models to Self-Debug</a> (B)</li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 11</font></b><br />Nov 10</td>
      <td>
      
        
          <div>
            Multimodal LM (A: Yuqing Wang, Jintao Lin)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec12.pdf">slides</a>]
            
          </div>
        
          <div>
            LM agent, language grounding (B: Yao Teng, Kaiyue Sun)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec13.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Paper Presentation
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2204.14198">Flamingo&#58; a Visual Language Model for Few-Shot Learning</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision </a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2210.03629">ReAct&#58; Synergizing Reasoning and Acting in Language Models</a> (B)</li>
              
              <li><a href="https://arxiv.org/abs/2004.10151">Experience Grounds Language</a> (B)</li>
              
            </ul>
          
            
              Recommended Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2301.12597">BLIP-2&#58; Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2304.08485">Visual Instruction Tuning</a> (A)</li>
              
              <li><a href="https://lilianweng.github.io/posts/2023-06-23-agent/">LLM Powered Autonomous Agents</a> (B)</li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 12</font></b><br />Nov 17</td>
      <td>
      
        
          <div>
            Tool use for agents (A: Yuer Yang, Li Maomao)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec14.pdf">slides</a>]
            
          </div>
        
          <div>
            Agent methods and applications (B: Jiabin Tang, Junchen Yan)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec15.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Paper Presentation
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2302.07842">Augmented Language Models&#58; A Survey</a> (A)</li>
              
              <li><a href="https://openreview.net/forum?id=hSyW5go0v8">Self-RAG&#58; Learning to Retrieve, Generate, and Critique through Self-Reflection</a> (A)</li>
              
              <li><a href="https://openreview.net/forum?id=B6pQxqUcT8">ToolChain*&#58; Efficient Action Space Navigation in Large Language Models with A* Search</a> (A)</li>
              
              <li><a href="https://openreview.net/forum?id=GEcwtMk1uA">Identifying the Risks of LM Agents with an LM-Emulated Sandbox</a> (B)</li>
              
              <li><a href="https://arxiv.org/abs/2311.07562">GPT-4V in Wonderland&#58; Large Multimodal Models for Zero-Shot Smartphone GUI Navigation</a> (B)</li>
              
              <li><a href="https://openreview.net/forum?id=hNhwSmtXRh">Lemur&#58; Harmonizing Natural Language and Code for Language Agents</a> (B)</li>
              
            </ul>
          
            
              Recommended Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2302.04761">Toolformer&#58; Language Models Can Teach Themselves to Use Tools</a> (A)</li>
              
              <li><a href="https://openreview.net/forum?id=fibxvahvs3">GAIA&#58; a benchmark for General AI Assistants</a> (B)</li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 13</font></b><br />Nov 24</td>
      <td>
      
        
          <div>
            Robotics and embodied interaction (Chirui Chang, Yihua Huang)
            
              <br />
              [<a href="/assets/courses/DATA8005-lec16.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Paper Presentation
            
            <ul>
              
              <li><a href="https://openreview.net/forum?id=OI3RoHoWAN">GenSim&#58; Generating Robotic Simulation Tasks via Large Language Models</a></li>
              
              <li><a href="https://openreview.net/forum?id=9pKtcJcMP3">Video Language Planning</a></li>
              
              <li><a href="https://arxiv.org/abs/2209.07753">Code as Policies&#58; Language Model Programs for Embodied Control</a></li>
              
              <li><a href="https://arxiv.org/abs/2204.01691">Do As I Can, Not As I Say&#58; Grounding Language in Robotic Affordances</a></li>
              
              <li><a href="https://arxiv.org/abs/2307.15818">RT-2&#58; Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</a></li>
              
              <li><a href="https://arxiv.org/abs/2305.16291">Voyager&#58; An Open-Ended Embodied Agent with Large Language Models</a></li>
              
            </ul>
          
            
              Recommended Readings
            
            <ul>
              
              <li><a href="https://www.annualreviews.org/doi/pdf/10.1146/annurev-control-101119-071628">Robots That Use Language</a></li>
              
              <li><a href="https://arxiv.org/abs/2206.08853">MineDojo&#58; Building Open-Ended Embodied Agents with Internet-Scale Knowledge</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 15</font></b><br />Dec 4</td>
      <td>
      
        
          <div>
            Invited talk <a href="https://yrf1.github.io/">Yi Ren Fung</a>
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 16</font></b><br />Dec 11</td>
      <td>
      
        
          <div>
            Invited talk <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week N/A</font></b><br />Jan 30</td>
      <td>
      
        
          <div>
            Invited talk <a href="https://dennyzhou.github.io/">Denny Zhou</a>
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    

</tbody>
</table>

    </div>

    <div class="footer">
      <div class="row">
        <div class="four columns">
          Tao Yu (余涛)
        </div>
        <div class="four columns">
          tao.yu.nlp [AT] gmail.com
        </div>
        <div class="four columns">
          <span onclick="window.open('https://twitter.com/taoyds')" style="cursor: pointer">
            <i class="fa fa-twitter" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://www.linkedin.com/in/tao-yu-b9b551a5/')" style="cursor: pointer">
            <i class="fa fa-linkedin-square" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://github.com/taoyds')" style="cursor: pointer">
            <i class="fa fa-github" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://scholar.google.com/citations?user=5_Fn5CIAAAAJ&hl')" style="cursor: pointer">
            <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
          </span>
        </div>
      </div>
    </div>

  </div>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-204806507-1', 'auto');
  ga('send', 'pageview');

</script>

  <!-- do not remove
  <span id="62cd7b7da1aff3196fdc26b60e396df9"></span>
   -->

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
