<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>COMP3361 | Tao Yu (余涛)</title>
  <meta name="description" content="Natural Language Processing">
  <meta name="author" content="Tao Yu">
  <meta property="og:title" content="Tao Yu" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://taoyds.github.io/" />
  <meta property="og:site_name" content="Tao Yu" />
  <link rel="canonical" href="https://taoyds.github.io/" />

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600' rel='stylesheet' type='text/css'>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton/normalize.css>
  <link rel="stylesheet" href=/libs/external/skeleton/skeleton.css>
  <link rel="stylesheet" href=/libs/custom/my_css.css>
  <link rel="stylesheet" href=/libs/custom/course.css>

  <!-- JQuery
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <script src=/libs/external/jquery-3.1.1.min.js></script>

  <!-- Font-Awesome
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/font-awesome-4.7.0/css/font-awesome.min.css>

  <!-- Academicons
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/academicons-1.8.6/css/academicons.min.css>

  <!-- Skeleton tabs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/skeleton_tabs/skeleton-tabs.css>
  <script src=/libs/external/skeleton_tabs/skeleton-tabs.js></script>

  <!-- Timeline
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href=/libs/external/timeline.css>

  <!-- Scripts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <!--<link rel="stylesheet" href=/libs/external/github-prettify-theme.css>-->
  <script src=/libs/custom/my_js.js></script>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href=/libs/icon.png>
  <link rel="shortcut icon" type="image/png" href=/libs/icon.png>

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="course">
      <h1 id="comp3361-natural-language-processing">COMP3361: Natural Language Processing</h1>

<h2 id="course-information">Course Information</h2>

<p>Instructor</p>

<ul>
  <li><a href="https://taoyds.github.io/">Tao Yu</a></li>
  <li>Office Hour: Wednesday 4 - 5pm @<a href="https://datascience.hku.hk/contact-us/#find-us">IDS P307H</a></li>
</ul>

<p>Lecture</p>

<ul>
  <li>Tuesday 9:30 am - 10:20 am and Friday 9:30 am - 11:20 am @<a href="https://its.hku.hk/teaching-space/kb132">KB132</a></li>
  <li><a href="https://docs.google.com/document/d/1aJUitHhGYnyOL3FTwDG88swyk4EoViDCSc6dlKbTocY/edit?usp=sharing">Course syllabus</a></li>
</ul>

<p>TA</p>

<ul>
  <li><a href="https://yihengxu.com/">Yiheng Xu</a> (Office Hour: Thursday 9 am - 10:15 am. <a href="https://calendly.com/yihengxu/comp3361-office-hour">Book here</a>)</li>
</ul>

<h2 id="course-description">Course Description</h2>

<p>Natural language processing (NLP) is the study of human language from a computational perspective. Over the past 20 years, the field of NLP has evolved significantly, primarily driven by advancements in statistical machine learning and deep learning. A notable recent breakthrough is the development of “pre-trained” language models, such as ChatGPT, which have substantially enhanced capabilities across various applications. This course is an introductory undergraduate-level course on natural language processing. In this class, we will cover recent developments on core techniques and modern advances in NLP, especially in the era of large language models. Students will gain the necessary skills and experience to understand, design, implement, and test large language models through lectures and assignments. We will potentially also host invited speakers for talks. </p>

<h2 id="prerequisites">Prerequisites</h2>

<ul>
  <li>COMP3314 or COMP3340; and MATH1853</li>
  <li>We require students to have prior knowledge undergraduate linear algebra, probability and statistics, machine learning, or deep learning. Familiarity with Python programming is required.</li>
</ul>

<h2 id="course-materials">Course Materials</h2>

<p>There is no required textbook for this course. <a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing (3rd ed. draft)</a> by Dan Jurafsky and James H. Martin (2023) and <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Natural Language Processing</a> by Jacob Eisenstein are recommended if you would like to read more about NLP. Readings from papers, blogs, tutorials, and book chapters will be posted on the course website. Textbook readings are assigned to complement the material discussed in lecture. You may find it useful to do these readings before lecture as preparation or after lecture to review, but you are not expected to know everything discussed in the textbook if it isn’t covered in lecture. Paper readings are intended to supplement the course material if you are interested in diving deeper on particular topics.</p>

<h2 id="grading">Grading</h2>

<ul>
  <li><strong>Assignments</strong>: 40%
    <ul>
      <li>Assignment 1 (A1): 20%</li>
      <li>Assignment 2 (A2): 20%</li>
    </ul>
  </li>
  <li><strong>Course project</strong>: 30%</li>
  <li><strong>In-class final exam</strong>: 25% (May 8, Wed, 9:30 am - 11:30 am @Rm 3 Library Ext.)</li>
  <li><strong>Class participation</strong>: 5%
    <ul>
      <li>Filling out course feedback surveys</li>
      <li>Participating in group discussions on Slack or in the class</li>
      <li>Attending potential guest speakers’ lectures, and so on</li>
    </ul>
  </li>
</ul>

<h2 id="course-schedule">Course Schedule</h2>

<table class="table">
<colgroup>
    <col style="width:10%" />
    <col style="width:20%" />
    <col style="width:40%" />
    <col style="width:10%" />
    <col style="width:10%" />
</colgroup>
<thead>
<tr>
    <th>Date</th>
    <th>Topic</th>
    <th>Material</th>
    <th>Event</th>
    <th>Due</th>
</tr>
</thead>
<tbody>
    
    
    
    <tr>
      <td><b><font color="red">Week 1</font></b><br />Jan 16</td>
      <td>
      
        
          <div>
            Introduction
            
              <br />
              [<a href="/assets/courses/COMP3361-lec01.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://www.amacad.org/publication/human-language-understanding-reasoning">Human Language Understanding &amp; Reasoning</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td>Jan 19</td>
      <td>
      
        
          <div>
            Language modeling (n-gram language models)
            
              <br />
              [<a href="/assets/courses/COMP3361-lec02.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf">M. Collins, Notes 1</a></li>
              
              <li><a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf">J&amp;M Ch. 7</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 2</font></b><br />Jan 23</td>
      <td>
      
        
          <div>
            Text classification
            
              <br />
              [<a href="/assets/courses/COMP3361-lec03.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://web.stanford.edu/~jurafsky/slp3/4.pdf">J&amp;M Ch. 4.1-4.6</a></li>
              
              <li><a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf">J&amp;M Ch. 5.1-5.8</a></li>
              
              <li><a href="https://www.cs.columbia.edu/~mcollins/ff2.pdf">M. Collins, Notes 2</a></li>
              
              <li><a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf">J&amp;M Ch. 6</a></li>
              
            </ul>
          
            
              Others
            
            <ul>
              
              <li><a href="https://docs.google.com/document/d/1LlnXoOblXwW3YX-0yG_5seTXJsb3kRdMMRYqs8Qqum4/edit?usp=sharing">Working with Google Colab</a></li>
              
            </ul>
          
        
    </td>
      <td><a href="https://github.com/ranpox/comp3361-spring2024/tree/main/assignments/A1">A1</a> out</td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td>Jan 26</td>
      <td>
      
        
          <div>
            Word embeddings 1
            
              <br />
              [<a href="/assets/courses/COMP3361-lec04.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf">J &amp; M 6.2-6.4, 6.6</a></li>
              
              <li><a href="https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">word2vec paper</a></li>
              
              <li><a href="https://aclanthology.org/D14-1162/">GloVe paper</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 3</font></b><br />Jan 30</td>
      <td>
      
        
          <div>
            Word embeddings 2
            
              <br />
              [<a href="/assets/courses/COMP3361-lec05.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf">J &amp; M 6.8, 6.10-6.12</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td>Feb 2</td>
      <td>
      
        
          <div>
            Neural language models: Overview, tokenization
            
              <br />
              [<a href="/assets/courses/COMP3361-lec06.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf">J &amp; M 7.3-7.5</a></li>
              
              <li><a href="https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/">Running example of using BERT</a></li>
              
              <li><a href="https://arxiv.org/abs/1508.07909">Byte-pair encoding paper</a></li>
              
            </ul>
          
            
              Others
            
            <ul>
              
              <li><a href="https://www.youtube.com/watch?v=zduSFxRajkE">Talk on the GPT Tokenizer by Andrej Karpathy</a></li>
              
              <li><a href="https://huggingface.co/docs/transformers/tokenizer_summary">Summary of the tokenizers with HuggingFace</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 4</font></b><br />Feb 6</td>
      <td>
      
        
          <div>
            Neural language models: RNNs
            
              <br />
              [<a href="/assets/courses/COMP3361-lec07.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://web.stanford.edu/~jurafsky/slp3/7.pdf">J &amp; M 7.3-7.5</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td>Feb 9</td>
      <td>
      
        
          <div>
            <a href="https://drive.google.com/file/d/1mRKeIkqyir7tDQVnCo3NgSyJf0ARp3hu/view?usp=sharing">Tutorial on PyTorch and Transformers</a>
            
              <br />
              [<a href="https://colab.research.google.com/drive/1VHyYjDPzO-5fqJuyBzBThwr_no6vV0J_?usp=sharing">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Other resources
            
            <ul>
              
              <li><a href="https://colab.research.google.com/drive/13HGy3-uIIy1KD_WFhG4nVrxJC-3nUUkP?usp=sharing">Stanford CS224N PyTorch Tutorial</a></li>
              
              <li><a href="https://huggingface.co/learn/nlp-course/chapter1/1">HuggingFace NLP Course</a></li>
              
              <li><a href="https://huggingface.co/docs/transformers/en/notebooks">Transformers Notebooks</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 5</font></b><br />Feb 13</td>
      <td>
      
        
          <div>
            No class
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td>Feb 16</td>
      <td>
      
        
          <div>
            No class
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 6</font></b><br />Feb 20</td>
      <td>
      
        
          <div>
            Neural language models: RNNs and LSTM
            
              <br />
              [<a href="/assets/courses/COMP3361-lec08.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://web.stanford.edu/~jurafsky/slp3/9.pdf">J &amp; M 9.1-9.3, 9.5</a></li>
              
              <li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
              
              <li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs">Understanding LSTM Networks</a></li>
              
            </ul>
          
        
    </td>
      <td><a href="https://github.com/ranpox/comp3361-spring2024/tree/main/assignments/A2">A2</a> out</td>
      <td>A1 <font color="red"><b>due</b></font></td>
    </tr>
    
    
    
    <tr>
      <td>Feb 23</td>
      <td>
      
        
          <div>
            Neural language models: Transformers
            
              <br />
              [<a href="/assets/courses/COMP3361-lec09.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://web.stanford.edu/~jurafsky/slp3/10.pdf">J &amp; M 10.1</a></li>
              
              <li><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a></li>
              
              <li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
              
              <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 7</font></b><br />Feb 27</td>
      <td>
      
        
          <div>
            Neural language models: Pretraining 1
            
              <br />
              [<a href="/assets/courses/COMP3361-lec10.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/pdf/1810.04805.pdf">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
              
              <li><a href="https://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co.</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td>Mar 1</td>
      <td>
      
        
          <div>
            Neural language models: Pretraining 2
            
              <br />
              [<a href="/assets/courses/COMP3361-lec11.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/pdf/1910.10683.pdf">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)</a></li>
              
              <li><a href="https://arxiv.org/pdf/2005.14165.pdf">Language Models are Few-Shot Learners (GPT-3)</a></li>
              
              <li><a href="https://arxiv.org/abs/2307.09288">Llama 2&#58; Open Foundation and Fine-Tuned Chat Models</a></li>
              
            </ul>
          
            
              Others
            
            <ul>
              
              <li><a href="https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=Q3k1Czf7LuA9">Colab on building GPT&#58; from scratch, in code, spelled out by Andrej Karpathy</a></li>
              
              <li><a href="https://jalammar.github.io/illustrated-gpt2/">The Illustrated GPT-2</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 8</font></b><br />Mar 5</td>
      <td>
      
        
          <div>
            No class
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td>Mar 8</td>
      <td>
      
        
          <div>
            No class
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 9</font></b><br />Mar 12</td>
      <td>
      
        
          <div>
            LLM prompting, in-context learning, scaling laws, emergent capacities 1
            
              <br />
              [<a href="/assets/courses/COMP3361-lec12.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>
              
            </ul>
          
            
              Others
            
            <ul>
              
              <li><a href="https://www.youtube.com/watch?v=bZQun8Y4L2A">State of GPT by Andrej Karpathy</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td>Mar 15</td>
      <td>
      
        
          <div>
            LLM prompting, in-context learning, scaling laws, emergent capacities 2
            
              <br />
              [<a href="/assets/courses/COMP3361-lec13.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2206.07682">Emergent Abilities of Large Language Models</a></li>
              
            </ul>
          
            
              Others
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2304.15004">Are Emergent Abilities of Large Language Models a Mirage?</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 10</font></b><br />Mar 19</td>
      <td>
      
        
          <div>
            Coding tutorial
            
              <br />
              [<a href="/assets/courses/COMP3361-tut02.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
              
              <li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Talk on building GPT&#58; from scratch, in code, spelled out by Andrej Karpathy</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td>Mar 22</td>
      <td>
      
        
          <div>
            Natural language generation with LLMs 1
            
              <br />
              [<a href="/assets/courses/COMP3361-lec14.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Others
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/1904.09751">The Curious Case of Neural Text Degeneration</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td>A2 <font color="red"><b>due</b></font></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 11</font></b><br />Mar 26</td>
      <td>
      
        
          <div>
            Natural language generation with LLMs 2
            
              <br />
              [<a href="/assets/courses/COMP3361-lec15.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td><a href="https://github.com/ranpox/comp3361-spring2024/tree/main/assignments/A3">A3 (originally the course project)</a> out</td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td>Mar 29</td>
      <td>
      
        
          <div>
            No class
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 12</font></b><br />Apr 2</td>
      <td>
      
        
          <div>
            Intro to advanced topics
            
              <br />
              [<a href="/assets/courses/COMP3361-lec16.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td>Apr 5</td>
      <td>
      
        
          <div>
            Code language models (by Ansong Ni, Yale)
            
              <br />
              [<a href="/assets/courses/COMP3361-talk01.pdf">slides</a>]
            
          </div>
        
          <div>
            Retrieval-augmented LMs (by Weijia Shi, UW)
            
              <br />
              [<a href="/assets/courses/COMP3361-talk02.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://instructor-embedding.github.io/">One Embedder, Any Task&#58; Instruction-Finetuned Text Embeddings</a></li>
              
              <li><a href="https://arxiv.org/abs/2301.12652">REPLUG&#58; Retrieval-Augmented Black-Box Language Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a></li>
              
              <li><a href="https://arxiv.org/abs/2308.12950">Code Llama&#58; Open Foundation Models for Code</a></li>
              
            </ul>
          
            
              Others
            
            <ul>
              
              <li><a href="https://acl2023-retrieval-lm.github.io/">Tutorial&#58; Retrieval-based Language Models and Applications</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 13</font></b><br />Apr 9</td>
      <td>
      
        
          <div>
            LLMs/VLMs as agents
            
              <br />
              [<a href="/assets/courses/COMP3361-lec17.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://lilianweng.github.io/posts/2023-06-23-agent/">LLM Powered Autonomous Agents</a></li>
              
              <li><a href="https://os-world.github.io/">OSWorld&#58; Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td>Apr 12</td>
      <td>
      
        
          <div>
            Solving Real-World Tasks with AI Agents (by Shuyan Zhou, CMU)
            
              <br />
              [<a href="/assets/courses/COMP3361-talk03.pdf">slides</a>]
            
          </div>
        
          <div>
            Instruction tuning for LLMs (by Yizhong Wang, UW)
            
              <br />
              [<a href="/assets/courses/COMP3361-talk04.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2109.01652">Finetuned Language Models Are Zero-Shot Learners</a></li>
              
              <li><a href="https://arxiv.org/abs/2212.10560">Self-Instruct&#58; Aligning Language Model with Self-Generated Instructions</a></li>
              
              <li><a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a></li>
              
              <li><a href="https://arxiv.org/abs/2211.10435">PAL&#58; Program-aided Language Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2211.10435">DocPrompting&#58; Generating Code by Retrieving the Docs</a></li>
              
            </ul>
          
            
              Others
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2307.13854">WebArena&#58; A Realistic Web Environment for Building Autonomous Agents</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 14</font></b><br />Apr 16</td>
      <td>
      
        
          <div>
            Efficient LM methods (by Bailin Wang, MIT)
            
              <br />
              [<a href="/assets/courses/COMP3361-talk05.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2106.09685">LoRA&#58; Low-Rank Adaptation of Large Language Models</a> (A)</li>
              
              <li><a href="https://arxiv.org/abs/2205.14135">FlashAttention&#58; Fast and Memory-Efficient Exact Attention with IO-Awareness</a> (B)</li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td>Apr 19</td>
      <td>
      
        
          <div>
            Principles of Reasoning&#58; Designing Compositional and Collaborative Generative AIs (by William Wang, UCSB, on Apr 18)
            
          </div>
        
          <div>
            LLM alignment (by Ruiqi Zhong, UC Berkeley)
            
              <br />
              [<a href="/assets/courses/COMP3361-talk06.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://arxiv.org/abs/2211.03540">Measuring Progress on Scalable Oversight for Large Language Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2206.05802">Self-critiquing models for assisting human evaluators</a></li>
              
              <li><a href="https://arxiv.org/abs/2109.10862">Recursively Summarizing Books with Human Feedback</a></li>
              
            </ul>
          
            
              Others
            
            <ul>
              
              <li><a href="https://ruiqizhong.substack.com/p/explaining-ai-alignment-as-an-nlper">Explaining AI Alignment as an NLPer and Why I am Working on It</a></li>
              
              <li><a href="https://www.anthropic.com/news/core-views-on-ai-safety">Core Views on AI Safety&#58; When, Why, What, and How</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 15</font></b><br />Apr 23</td>
      <td>
      
        
          <div>
            Multimodal language models/VLMs (by Yushi Hu, UW)
            
              <br />
              [<a href="/assets/courses/COMP3361-talk07.pdf">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://openai.com/research/clip">CLIP&#58; Connecting text and images</a></li>
              
              <li><a href="https://huggingface.co/blog/vision_language_pretraining">A Dive into Vision-Language Models</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td>Apr 26</td>
      <td>
      
        
          <div>
            Robotics in the era of LLM/VLMs (by Ted Xiao, Google DeepMind)
            
              <br />
              [<a href="https://docs.google.com/presentation/d/1UuRtZzoY_jq6z98gMwW-SGfCpVYK0UnVnP1OZdzHLEI/edit?usp=sharing">slides</a>]
            
          </div>
        
      
      </td>
      <td>
        
          
            
              Readings
            
            <ul>
              
              <li><a href="https://say-can.github.io/">Do As I Can, Not As I Say&#58; Grounding Language in Robotic Affordances</a></li>
              
              <li><a href="https://robotics-transformer1.github.io/">RT-1&#58; Robotics Transformer for Real-World Control at Scale</a></li>
              
            </ul>
          
            
              Others
            
            <ul>
              
              <li><a href="https://robotics-transformer2.github.io/">RT-2&#58; Vision-Language-Action Models</a></li>
              
              <li><a href="https://arxiv.org/abs/2311.14379">Robot Learning in the Era of Foundation Models&#58; A Survey</a></li>
              
            </ul>
          
        
    </td>
      <td></td>
      <td></td>
    </tr>
    
    
    
    <tr>
      <td><b><font color="red">Week 16</font></b><br />Apr 30</td>
      <td>
      
        
          <div>
            No class (Revision)
            
          </div>
        
      
      </td>
      <td>
        
    </td>
      <td></td>
      <td>Course project <font color="red"><b>due</b></font></td>
    </tr>
    
    

</tbody>
</table>

    </div>

    <div class="footer">
      <div class="row">
        <div class="four columns">
          Tao Yu (余涛)
        </div>
        <div class="four columns">
          tao.yu.nlp [AT] gmail.com
        </div>
        <div class="four columns">
          <span onclick="window.open('https://twitter.com/taoyds')" style="cursor: pointer">
            <i class="fa fa-twitter" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://www.linkedin.com/in/tao-yu-b9b551a5/')" style="cursor: pointer">
            <i class="fa fa-linkedin-square" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://github.com/taoyds')" style="cursor: pointer">
            <i class="fa fa-github" aria-hidden="true"></i>
          </span>
          <span onclick="window.open('https://scholar.google.com/citations?user=5_Fn5CIAAAAJ&hl')" style="cursor: pointer">
            <i class="ai ai-google-scholar ai-lg" aria-hidden="true"></i>
          </span>
        </div>
      </div>
    </div>

  </div>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-204806507-1', 'auto');
  ga('send', 'pageview');

</script>

  <!-- do not remove
  <span id="62cd7b7da1aff3196fdc26b60e396df9"></span>
   -->

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
